{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:55:11.578878Z",
     "start_time": "2025-12-02T09:55:11.574624Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "from typing import List, Dict\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a5727f4cdf872",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T09:55:11.831574Z",
     "start_time": "2025-12-02T09:55:11.816879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize with your API key\n",
    "API_KEY = ''\n",
    "NO_SYN=[]\n",
    "SYN = {}\n",
    "PARSED=[]\n",
    "class UMLSClient:\n",
    "    \"\"\"Client for interacting with UMLS REST API\"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_uri = \"https://uts-ws.nlm.nih.gov/rest\"\n",
    "        self.service_ticket = None\n",
    "\n",
    "    def get_service_ticket(self):\n",
    "        \"\"\"Get a service ticket for authentication\"\"\"\n",
    "        tgt_url = f\"https://utslogin.nlm.nih.gov/cas/v1/api-key\"\n",
    "        params = {'apikey': self.api_key}\n",
    "\n",
    "        try:\n",
    "            r = requests.post(tgt_url, data=params)\n",
    "            r.raise_for_status()\n",
    "\n",
    "            # Extract TGT from response\n",
    "            response_text = r.text\n",
    "            tgt_match = re.search(r'action=\"(.*?)\"', response_text)\n",
    "            if tgt_match:\n",
    "                tgt = tgt_match.group(1)\n",
    "                # Get service ticket\n",
    "                st_params = {'service': 'http://umlsks.nlm.nih.gov'}\n",
    "                st_response = requests.post(tgt, data=st_params)\n",
    "                return st_response.text.strip()\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting service ticket: {e}\")\n",
    "            return None\n",
    "\n",
    "    def search_term(self, term: str, search_type: str = \"exact\", sabs: str = \"MTH\"):\n",
    "        \"\"\"\n",
    "        Search UMLS for a term\n",
    "\n",
    "        Args:\n",
    "            term: The term to search for\n",
    "            search_type: Type of search (exact, words, leftTruncation, rightTruncation, approximate)\n",
    "            sabs: Source vocabularies (comma-separated)\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing CUI, name, and source vocabulary\n",
    "        \"\"\"\n",
    "        # Get fresh service ticket\n",
    "        ticket = self.get_service_ticket()\n",
    "        if not ticket:\n",
    "            return []\n",
    "\n",
    "        search_url = f\"{self.base_uri}/search/current\"\n",
    "        params = {\n",
    "            'string': term,\n",
    "            'searchType': search_type,\n",
    "            'sabs': sabs,\n",
    "            'ticket': ticket\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(search_url, params=params)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            results = []\n",
    "            data = response.json()\n",
    "\n",
    "            if 'result' in data and 'results' in data['result']:\n",
    "                for result in data['result']['results']:\n",
    "                    results.append({\n",
    "                        'cui': result.get('ui', ''),\n",
    "                        'name': result.get('name', ''),\n",
    "                        'rootSource': result.get('rootSource', '')\n",
    "                    })\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for term '{term}': {e}\")\n",
    "            return []\n",
    "def get_umls_synonyms(term: str, api_key: str, search_type: str = \"exact\", sabs: str = \"MTH\") -> dict:\n",
    "    \"\"\"\n",
    "    Get all synonyms for a given term from UMLS\n",
    "\n",
    "    Args:\n",
    "        term: The term to search for\n",
    "        api_key: UMLS API key\n",
    "        search_type: Type of search (exact, words, leftTruncation, rightTruncation, approximate)\n",
    "        sabs: Source vocabularies (comma-separated). Default \"MTH\" for Metathesaurus\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "            - 'original_term': The input term\n",
    "            - 'cuis': List of CUIs found\n",
    "            - 'synonyms': Set of all unique synonyms across all CUIs\n",
    "            - 'details': List of dicts with CUI-specific synonym information\n",
    "    \"\"\"\n",
    "    client = UMLSClient(api_key)\n",
    "\n",
    "    # Search for the term to get CUIs\n",
    "    search_results = client.search_term(term, search_type, sabs)\n",
    "\n",
    "    if not search_results:\n",
    "        return {\n",
    "            'original_term': term,\n",
    "            'cuis': [],\n",
    "            'synonyms': set(),\n",
    "            'details': []\n",
    "        }\n",
    "\n",
    "    all_synonyms = set()\n",
    "    cui_details = []\n",
    "    cuis = []\n",
    "\n",
    "    # For each CUI found, get all atoms (synonyms)\n",
    "    for result in search_results:\n",
    "        cui = result['cui']\n",
    "        cuis.append(cui)\n",
    "\n",
    "        # Get service ticket for this request\n",
    "        ticket = client.get_service_ticket()\n",
    "        if not ticket:\n",
    "            continue\n",
    "\n",
    "        # Get atoms (terms/synonyms) for this CUI\n",
    "        atoms_url = f\"{client.base_uri}/content/current/CUI/{cui}/atoms\"\n",
    "        params = {\n",
    "            'ticket': ticket,\n",
    "            'sabs': sabs,\n",
    "            'pageSize': 1000  # Adjust if you need more results\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(atoms_url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            cui_synonyms = set()\n",
    "\n",
    "            if 'result' in data:\n",
    "                for atom in data['result']:\n",
    "                    synonym = atom.get('name', '')\n",
    "                    if synonym:\n",
    "                        cui_synonyms.add(synonym)\n",
    "                        all_synonyms.add(synonym)\n",
    "\n",
    "            cui_details.append({\n",
    "                'cui': cui,\n",
    "                'preferred_name': result['name'],\n",
    "                'source': result['rootSource'],\n",
    "                'synonyms': list(cui_synonyms)\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting atoms for CUI {cui}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        'original_term': term,\n",
    "        'cuis': cuis,\n",
    "        'synonyms': all_synonyms,\n",
    "        'details': cui_details\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613919062fa48c5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T10:02:47.593453Z",
     "start_time": "2025-12-02T10:02:47.553976Z"
    }
   },
   "outputs": [],
   "source": [
    "class MedicalNotesRAG:\n",
    "    def __init__(self, df: pd.DataFrame, chunk_size: int = 400,auth_token = '',name_file = 'rag_file_prompt.csv'):\n",
    "        \"\"\"\n",
    "        Initialize RAG system for medical notes.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame with columns [text, note_id, hadm_id, criterion, question_type, question, answer, not_specified]\n",
    "            chunk_size: Target number of tokens per chunk\n",
    "            top_p: Top-p threshold for retrieval (cumulative probability)\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.chunk_size = chunk_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.file_name = name_file\n",
    "        # Initialize models\n",
    "        print(\"Loading Bio_ClinicalBERT for embeddings...\")\n",
    "        self.embedding_model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "        self.embedding_tokenizer = AutoTokenizer.from_pretrained(self.embedding_model_name)\n",
    "        self.embedding_model = AutoModel.from_pretrained(self.embedding_model_name).to(self.device)\n",
    "        self.embedding_model.eval()\n",
    "        # Storage\n",
    "        self.notes_df = None\n",
    "        self.chunks_df = None\n",
    "        self.queries_df = None\n",
    "        self.vector_index = None\n",
    "        self.chunk_embeddings = None\n",
    "        self.embedding_dim = 768  # BioBERT dimension\n",
    "\n",
    "    #Extract unique medical notes\n",
    "    def extract_notes(self):\n",
    "        # Group by note_id to get unique notes\n",
    "        notes_data = []\n",
    "        for note_id, group in self.df.groupby('note_id'):\n",
    "            hadm_id = group['hadm_id'].iloc[0]\n",
    "            text = group['text'].iloc[0]\n",
    "            notes_data.append({\n",
    "                'note_id': note_id,\n",
    "                'hadm_id': hadm_id,\n",
    "                'text': text\n",
    "            })\n",
    "\n",
    "        self.notes_df = pd.DataFrame(notes_data)\n",
    "        print(f\"✓ Extracted {len(self.notes_df)} unique notes\")\n",
    "        print(f\"✓ From {self.notes_df['hadm_id'].nunique()} unique patients\")\n",
    "\n",
    "    #Utility for splitting text into chunks\n",
    "    def _chunk_text(self, text: str, note_id: str, hadm_id: int) -> List[Dict]:\n",
    "        #Split text into overlapping chunks.\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_tokens = 0\n",
    "        chunk_id = 0\n",
    "    \n",
    "        for sentence in sentences:\n",
    "            tokens = len(self.embedding_tokenizer.encode(sentence))\n",
    "    \n",
    "            if current_tokens + tokens > self.chunk_size and current_chunk:\n",
    "                chunk_text = '. '.join(current_chunk) + '.'\n",
    "                chunks.append({\n",
    "                    'chunk_id': f\"{note_id}_chunk_{chunk_id}\",\n",
    "                    'note_id': note_id,\n",
    "                    'hadm_id': hadm_id,\n",
    "                    'chunk_text': chunk_text\n",
    "                })\n",
    "                chunk_id += 1\n",
    "                \n",
    "                # Overlap: keep last sentence\n",
    "                current_chunk = [current_chunk[-1]] if current_chunk else []\n",
    "                current_tokens = len(self.embedding_tokenizer.encode(current_chunk[0])) if current_chunk else 0\n",
    "    \n",
    "            current_chunk.append(sentence)\n",
    "            current_tokens += tokens\n",
    "    \n",
    "        # Add remaining chunk\n",
    "        if current_chunk:\n",
    "            chunk_text = '. '.join(current_chunk) + '.'\n",
    "            chunks.append({\n",
    "                'chunk_id': f\"{note_id}_chunk_{chunk_id}\",\n",
    "                'note_id': note_id,\n",
    "                'hadm_id': hadm_id,\n",
    "                'chunk_text': chunk_text\n",
    "            })\n",
    "    \n",
    "        return chunks\n",
    " \n",
    "    #Create chunks from medical notes\n",
    "    def create_chunks(self):\n",
    "        all_chunks = []\n",
    "        for _, row in self.notes_df.iterrows():\n",
    "            chunks = self._chunk_text(row['text'], row['note_id'], row['hadm_id'])\n",
    "            all_chunks.extend(chunks)\n",
    "        self.chunks_df = pd.DataFrame(all_chunks)\n",
    "        print(f\"✓ Created {len(self.chunks_df)} chunks from {len(self.notes_df)} notes\")\n",
    "        print(f\"✓ Average chunks per note: {len(self.chunks_df) / len(self.notes_df):.2f}\")\n",
    "\n",
    "    #Utility to generate vector embeddings\n",
    "    def _embed_text(self, text: str) -> np.ndarray:\n",
    "        #Generate embeddings using BioBERT\n",
    "        \n",
    "        inputs = self.embedding_tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedding_model(**inputs)\n",
    "            # Mean pooling\n",
    "            last_hidden = outputs.last_hidden_state\n",
    "            mask = inputs['attention_mask'].unsqueeze(-1).float()\n",
    "            pooled = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
    "\n",
    "        return pooled.cpu().numpy()[0]\n",
    "    \n",
    "    #Generate embeddings for chunks.\n",
    "    def create_embeddings(self):\n",
    "        embeddings = []\n",
    "        for idx, row in self.chunks_df.iterrows():\n",
    "            emb = self._embed_text(row['chunk_text'])\n",
    "            embeddings.append(emb)\n",
    "            if (idx + 1) % 50 == 0:\n",
    "                print(f\"  Embedded {idx + 1}/{len(self.chunks_df)} chunks\")\n",
    "        self.chunk_embeddings = np.array(embeddings).astype('float32')\n",
    "        print(f\"✓ Generated {len(self.chunk_embeddings)} vector embeddings\")\n",
    "\n",
    "    #Utility to create dataframe containing queries, hadm_id, question_type, and answers.\n",
    "    def create_queries_dataframe(self):\n",
    "        queries_data = []\n",
    "        for _, row in self.df.iterrows():\n",
    "            queries_data.append({\n",
    "                'hadm_id': row['hadm_id'],\n",
    "                'note_id': row['note_id'],\n",
    "                'question': row['question'],\n",
    "                'question_type': row['question_type'],\n",
    "                'answer': row['answer'],\n",
    "                'criterion': row.get('criterion', ''),\n",
    "                'not_specified': row.get('not_specified', False)\n",
    "            })\n",
    "\n",
    "        self.queries_df = pd.DataFrame(queries_data)\n",
    "        print(f\"✓ Created queries dataframe with {len(self.queries_df)} questions\")\n",
    "        print(f\"✓ Question types: {self.queries_df['question_type'].value_counts().to_dict()}\")\n",
    "        print(f\"✓ Questions per patient (avg): {len(self.queries_df) / self.queries_df['hadm_id'].nunique():.2f}\")\n",
    "        return self.queries_df\n",
    "\n",
    "    # Retrieve chunks using top-p / top-k filtering (with cosine similarity)\n",
    "    def retrieve_with_top_k(self, query, hadm_id, top_k) -> List[Dict]:\n",
    "        # Get query embedding\n",
    "        query_emb = self._embed_text(query).reshape(1, -1).astype('float32')\n",
    "\n",
    "        # Filter chunks to only this patient\n",
    "        patient_chunk_indices = self.chunks_df[self.chunks_df['hadm_id'] == hadm_id].index.tolist()\n",
    "\n",
    "        if len(patient_chunk_indices) == 0:\n",
    "            print(f\"No chunks found for patient {hadm_id}\")\n",
    "            return []\n",
    "\n",
    "        # Get embeddings for this patient's chunks\n",
    "        patient_embeddings = self.chunk_embeddings[patient_chunk_indices].astype('float32')\n",
    "\n",
    "        # ---- COSINE SIMILARITY SETUP ----\n",
    "        faiss.normalize_L2(patient_embeddings)\n",
    "        faiss.normalize_L2(query_emb)\n",
    "\n",
    "        temp_index = faiss.IndexFlatIP(self.embedding_dim)\n",
    "        temp_index.add(patient_embeddings)\n",
    "\n",
    "        # distances are actually cosine similarities in [-1, 1]\n",
    "        similarities, indices = temp_index.search(query_emb, len(patient_chunk_indices))\n",
    "\n",
    "        sim = similarities[0]\n",
    "        neighbor_local = indices[0]\n",
    "\n",
    "        # Convert similarities to probabilities using softmax\n",
    "        exp_similarities = np.exp(sim - np.max(sim))\n",
    "        probabilities = exp_similarities / np.sum(exp_similarities)\n",
    "\n",
    "        # Sort by probability (descending) in neighbor-rank space\n",
    "        sorted_ranks = np.argsort(probabilities)[::-1]\n",
    "        sorted_probs = probabilities[sorted_ranks]\n",
    "\n",
    "        # ----- Selection logic -----\n",
    "        # if self.top == 'p':\n",
    "        #     cumulative_probs = np.cumsum(sorted_probs)\n",
    "        #     top_p_mask = cumulative_probs <= self.top_p\n",
    "        #     if not np.any(top_p_mask):\n",
    "        #         top_p_mask[0] = True\n",
    "        #     selected_sorted_positions = np.where(top_p_mask)[0]\n",
    "\n",
    "        k = min(top_k, len(sorted_ranks))\n",
    "        selected_sorted_positions = np.arange(k)\n",
    "\n",
    "        selected_ranks = sorted_ranks[selected_sorted_positions]\n",
    "        selected_probs = sorted_probs[selected_sorted_positions]\n",
    "\n",
    "        results = []\n",
    "        for rank, prob in zip(selected_ranks, selected_probs):\n",
    "            local_idx = neighbor_local[rank]\n",
    "            global_idx = patient_chunk_indices[local_idx]\n",
    "\n",
    "            chunk_data = self.chunks_df.iloc[global_idx].to_dict()\n",
    "\n",
    "            cosine_sim = float(sim[rank])\n",
    "            cosine_dist = 1.0 - cosine_sim\n",
    "\n",
    "            chunk_data['chunk_idx'] = int(global_idx)\n",
    "            chunk_data['distance'] = cosine_dist\n",
    "            chunk_data['similarity'] = cosine_sim\n",
    "            chunk_data['probability'] = float(prob)\n",
    "            results.append(chunk_data)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def keyword_search(self, query,criterion, hadm_id, top_k) -> List[Dict]:\n",
    "        # Filter to this patient's chunks\n",
    "        patient_mask = self.chunks_df['hadm_id'] == hadm_id\n",
    "        patient_chunks = self.chunks_df[patient_mask].copy()\n",
    "\n",
    "        if patient_chunks.empty:\n",
    "            print(f\"No chunks found for patient {hadm_id}\")\n",
    "            return []\n",
    "\n",
    "        # Tokenize query into terms\n",
    "        # terms = re.findall(r\"\\w+\", query.lower())\n",
    "        # stopwords = {\"a\", \"an\", \"the\", \"of\", \"in\", \"on\", \"at\", \"to\", \"and\", \"or\"}\n",
    "        # terms = [t for t in terms if t not in stopwords]\n",
    "        criteria = criterion.replace('_', ' ').replace('-', ' ')\n",
    "        terms = [criteria,criteria.lower()]\n",
    "        if criteria not in NO_SYN and criteria not in PARSED:\n",
    "            extra_syn = get_umls_synonyms(criteria, API_KEY)\n",
    "        elif criteria in PARSED:\n",
    "            extra_syn = {'synonyms': SYN[criteria]}\n",
    "        else:\n",
    "            extra_syn = {'synonyms': []}\n",
    "        if len(extra_syn['synonyms'])>0:\n",
    "            list_syn = []\n",
    "            for syn in extra_syn['synonyms']:\n",
    "                terms.append(syn)\n",
    "                list_syn.append(syn)\n",
    "            PARSED.append(criteria)\n",
    "            SYN[criteria] = list_syn\n",
    "        else:\n",
    "            # print(f\"No synonyms found for {criteria} from UMLS\")\n",
    "            NO_SYN.append(criteria)\n",
    "        if not terms:\n",
    "            # print(\"No meaningful terms in query; returning empty keyword results.\")\n",
    "            return []\n",
    "\n",
    "        def score_text(text):\n",
    "            if not isinstance(text, str):\n",
    "                text = str(text)\n",
    "            text_l = text.lower()\n",
    "            return sum(term in text_l for term in terms)\n",
    "\n",
    "        # score each chunk (uses 'chunk_text' column)\n",
    "        patient_chunks['keyword_score'] = patient_chunks['chunk_text'].apply(score_text)\n",
    "\n",
    "        # keep only chunks with at least one hit\n",
    "        patient_chunks = patient_chunks[patient_chunks['keyword_score'] > 0]\n",
    "        if patient_chunks.empty:\n",
    "            # print(f\"No keyword hits for query '{query}' in patient {hadm_id}\")\n",
    "            return []\n",
    "\n",
    "        # sort by keyword score descending\n",
    "        patient_chunks = patient_chunks.sort_values(\n",
    "            by=['keyword_score'],\n",
    "            ascending=[False]\n",
    "        )\n",
    "\n",
    "        # take top_k\n",
    "        top_k = min(top_k, len(patient_chunks))\n",
    "        patient_chunks = patient_chunks.head(top_k)\n",
    "\n",
    "        results = []\n",
    "        for idx, row in patient_chunks.iterrows():\n",
    "            item = row.to_dict()\n",
    "            item['chunk_idx'] = int(idx)\n",
    "            item['keyword_score'] = int(row['keyword_score'])\n",
    "            results.append(item)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def retrieve_hybrid(self, query, criterion, hadm_id, kw_top_k, sem_top_k, alpha, final_top_k):\n",
    "        \"\"\"\n",
    "        Hybrid retrieval = semantic (FAISS cosine) + keyword search.\n",
    "\n",
    "        alpha = 1.0 -> pure semantic\n",
    "        alpha = 0.0 -> pure keyword\n",
    "        \"\"\"\n",
    "\n",
    "        # --- 1. Semantic retrieval (temporarily widen top_k) ---\n",
    "        semantic_results = self.retrieve_with_top_k(query, hadm_id, sem_top_k)\n",
    "\n",
    "        # --- 2. Keyword retrieval ---\n",
    "        keyword_results = self.keyword_search(query,criterion, hadm_id, kw_top_k)\n",
    "\n",
    "        # Fallbacks if one side is empty\n",
    "        if not semantic_results and not keyword_results:\n",
    "            return []\n",
    "        if not semantic_results:\n",
    "            return keyword_results[:final_top_k]\n",
    "        if not keyword_results:\n",
    "            return semantic_results[:final_top_k]\n",
    "\n",
    "        # --- 3. Build a map by chunk_idx to fuse scores ---\n",
    "        hybrid_map = {}  # chunk_idx -> {\"sem_score\": ..., \"kw_score\": ...}\n",
    "\n",
    "        # semantic probability already ~[0,1]\n",
    "        for r in semantic_results:\n",
    "            cid = r['chunk_idx']\n",
    "            entry = hybrid_map.get(cid, {\"sem_score\": 0.0, \"kw_score\": 0.0})\n",
    "            entry[\"sem_score\"] = max(entry[\"sem_score\"], float(r.get(\"probability\", 0.0)))\n",
    "            hybrid_map[cid] = entry\n",
    "\n",
    "        # keyword score is integer count, we'll normalize later\n",
    "        for r in keyword_results:\n",
    "            cid = r['chunk_idx']\n",
    "            entry = hybrid_map.get(cid, {\"sem_score\": 0.0, \"kw_score\": 0.0})\n",
    "            entry[\"kw_score\"] = max(entry[\"kw_score\"], float(r.get(\"keyword_score\", 0.0)))\n",
    "            hybrid_map[cid] = entry\n",
    "\n",
    "        # --- 4. Normalize keyword scores and compute combined score ---\n",
    "        max_kw = max((v[\"kw_score\"] for v in hybrid_map.values()), default=0.0)\n",
    "        if max_kw <= 0:\n",
    "            max_kw = 1.0  # avoid division by zero\n",
    "\n",
    "        results = []\n",
    "        for cid, scores in hybrid_map.items():\n",
    "            sem_score = scores[\"sem_score\"]                     # [0,1]\n",
    "            kw_norm = scores[\"kw_score\"] / max_kw               # [0,1]\n",
    "\n",
    "            combined = alpha * sem_score + (1.0 - alpha) * kw_norm\n",
    "\n",
    "            row = self.chunks_df.iloc[cid].to_dict()\n",
    "            row['chunk_idx'] = int(cid)\n",
    "            row['semantic_score'] = float(sem_score)\n",
    "            row['keyword_score'] = float(scores[\"kw_score\"])\n",
    "            row['combined_score'] = float(combined)\n",
    "\n",
    "            results.append(row)\n",
    "\n",
    "        # --- 5. Sort by combined_score and return top-N ---\n",
    "        results.sort(key=lambda x: x['combined_score'], reverse=True)\n",
    "        return results[:final_top_k]\n",
    "        \n",
    "    def create_prompt(self, question: str, question_type: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Create prompt based on question type with strict output format rules.\n",
    "\n",
    "        \"\"\"\n",
    "        if question_type.lower() == 'yes':\n",
    "            rule = \"You must answer with ONLY 'yes' or 'no'. No other words or explanations.\"\n",
    "            format_instruction = \"Answer (yes/no):\"\n",
    "        else:  # numeric\n",
    "            rule = \"You must answer with ONLY a number (with decimals if needed). No units, no other words.\"\n",
    "            format_instruction = \"Answer (number only):\"\n",
    "\n",
    "        prompt = f\"\"\"Medical Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            RULE: {rule}\n",
    "\n",
    "            {format_instruction}\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def save_prompt(self, kw_top_k, sem_top_k, alpha, final_top_k) -> Dict:\n",
    "        \"\"\"\"\"\n",
    "        Saves a .csv file with query, true_answer and prompt created using the patients medical notes - for every patient and every query - 2300 rows\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "        prompts_data = []\n",
    "        patient_results = defaultdict(lambda: {'correct': [], 'total': 0, 'numeric_errors': []})\n",
    "        question_results = defaultdict(lambda: {'correct': [], 'total': 0})\n",
    "\n",
    "        # LIMIT TO FIRST 10 QUERIES\n",
    "        queries_to_process = self.queries_df.head(2300)\n",
    "        print(f'Queries to process: {len(queries_to_process)}')\n",
    "        # Use tqdm with the dataframe iteration\n",
    "        with tqdm(total=len(queries_to_process), desc=\"Evaluating queries\", unit=\"query\") as pbar:\n",
    "            for idx, row in queries_to_process.iterrows():\n",
    "                question = row['question']\n",
    "                true_answer = str(row['answer']).strip().lower()\n",
    "                hadm_id = row['hadm_id']\n",
    "                question_type = row['question_type']\n",
    "                criterion = row['criterion']\n",
    "\n",
    "                # Retrieve chunks with top-p filtering (patient-isolated)\n",
    "                retrieved_chunks = self.retrieve_hybrid(\n",
    "                    question,\n",
    "                    criterion,\n",
    "                    hadm_id,\n",
    "                    kw_top_k,\n",
    "                    sem_top_k,\n",
    "                    alpha,\n",
    "                    final_top_k)\n",
    "\n",
    "                if len(retrieved_chunks) == 0:\n",
    "                    predicted_answer = \"no_data\"\n",
    "                    is_correct = False\n",
    "                    percentage_error = None\n",
    "                    prompt = \"NO DATA - No chunks retrieved\"\n",
    "                else:\n",
    "                    # Create context from retrieved chunks\n",
    "                    context = \"\\n\\n\".join([chunk['chunk_text'] for chunk in retrieved_chunks])\n",
    "\n",
    "                    # Create prompt based on question type\n",
    "                    prompt = self.create_prompt(question, question_type, context)\n",
    "\n",
    "\n",
    "                # Store prompt data\n",
    "                prompts_data.append({\n",
    "                    'query_index': idx,\n",
    "                    'hadm_id': hadm_id,\n",
    "                    'question': question,\n",
    "                    'criterion': criterion,\n",
    "                    'question_type': question_type,\n",
    "                    'true_answer': true_answer,\n",
    "                    'prompt': prompt,\n",
    "                    'num_chunks_retrieved': len(retrieved_chunks)\n",
    "                })\n",
    "\n",
    "\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Save prompts to CSV\n",
    "        prompts_df = pd.DataFrame(prompts_data)\n",
    "        prompts_df.to_csv(self.file_name, index=False)\n",
    "        print(f\"\\n✓ Saved all prompts to {self.file_name}\")\n",
    "\n",
    "\n",
    "        return prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59e89cb33e4cf7ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T21:22:50.196390Z",
     "start_time": "2025-12-02T19:52:52.685497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset shape: (2300, 8)\n",
      "Columns: ['text', 'note_id', 'hadm_id', 'criterion', 'question_type', 'question', 'answer', 'not_specified']\n",
      "Unique notes: 100\n",
      "Unique patients: 100\n",
      "fixed_RAG_prompt_save_c_50_k_15.csv\n",
      "Loading Bio_ClinicalBERT for embeddings...\n",
      "================================================================================\n",
      "STEP 1: EXTRACT UNIQUE MEDICAL NOTES\n",
      "================================================================================\n",
      "✓ Extracted 100 unique notes\n",
      "✓ From 100 unique patients\n",
      "\n",
      "================================================================================\n",
      "STEP 2: CREATING CHUNKS FROM NOTES\n",
      "================================================================================\n",
      "✓ Created 13829 chunks from 100 notes\n",
      "✓ Average chunks per note: 138.29\n",
      "\n",
      "================================================================================\n",
      "STEP 3: CREATING EMBEDDINGS FOR CHUNKS\n",
      "================================================================================\n",
      "  Embedded 50/13829 chunks\n",
      "  Embedded 100/13829 chunks\n",
      "  Embedded 150/13829 chunks\n",
      "  Embedded 200/13829 chunks\n",
      "  Embedded 250/13829 chunks\n",
      "  Embedded 300/13829 chunks\n",
      "  Embedded 350/13829 chunks\n",
      "  Embedded 400/13829 chunks\n",
      "  Embedded 450/13829 chunks\n",
      "  Embedded 500/13829 chunks\n",
      "  Embedded 550/13829 chunks\n",
      "  Embedded 600/13829 chunks\n",
      "  Embedded 650/13829 chunks\n",
      "  Embedded 700/13829 chunks\n",
      "  Embedded 750/13829 chunks\n",
      "  Embedded 800/13829 chunks\n",
      "  Embedded 850/13829 chunks\n",
      "  Embedded 900/13829 chunks\n",
      "  Embedded 950/13829 chunks\n",
      "  Embedded 1000/13829 chunks\n",
      "  Embedded 1050/13829 chunks\n",
      "  Embedded 1100/13829 chunks\n",
      "  Embedded 1150/13829 chunks\n",
      "  Embedded 1200/13829 chunks\n",
      "  Embedded 1250/13829 chunks\n",
      "  Embedded 1300/13829 chunks\n",
      "  Embedded 1350/13829 chunks\n",
      "  Embedded 1400/13829 chunks\n",
      "  Embedded 1450/13829 chunks\n",
      "  Embedded 1500/13829 chunks\n",
      "  Embedded 1550/13829 chunks\n",
      "  Embedded 1600/13829 chunks\n",
      "  Embedded 1650/13829 chunks\n",
      "  Embedded 1700/13829 chunks\n",
      "  Embedded 1750/13829 chunks\n",
      "  Embedded 1800/13829 chunks\n",
      "  Embedded 1850/13829 chunks\n",
      "  Embedded 1900/13829 chunks\n",
      "  Embedded 1950/13829 chunks\n",
      "  Embedded 2000/13829 chunks\n",
      "  Embedded 2050/13829 chunks\n",
      "  Embedded 2100/13829 chunks\n",
      "  Embedded 2150/13829 chunks\n",
      "  Embedded 2200/13829 chunks\n",
      "  Embedded 2250/13829 chunks\n",
      "  Embedded 2300/13829 chunks\n",
      "  Embedded 2350/13829 chunks\n",
      "  Embedded 2400/13829 chunks\n",
      "  Embedded 2450/13829 chunks\n",
      "  Embedded 2500/13829 chunks\n",
      "  Embedded 2550/13829 chunks\n",
      "  Embedded 2600/13829 chunks\n",
      "  Embedded 2650/13829 chunks\n",
      "  Embedded 2700/13829 chunks\n",
      "  Embedded 2750/13829 chunks\n",
      "  Embedded 2800/13829 chunks\n",
      "  Embedded 2850/13829 chunks\n",
      "  Embedded 2900/13829 chunks\n",
      "  Embedded 2950/13829 chunks\n",
      "  Embedded 3000/13829 chunks\n",
      "  Embedded 3050/13829 chunks\n",
      "  Embedded 3100/13829 chunks\n",
      "  Embedded 3150/13829 chunks\n",
      "  Embedded 3200/13829 chunks\n",
      "  Embedded 3250/13829 chunks\n",
      "  Embedded 3300/13829 chunks\n",
      "  Embedded 3350/13829 chunks\n",
      "  Embedded 3400/13829 chunks\n",
      "  Embedded 3450/13829 chunks\n",
      "  Embedded 3500/13829 chunks\n",
      "  Embedded 3550/13829 chunks\n",
      "  Embedded 3600/13829 chunks\n",
      "  Embedded 3650/13829 chunks\n",
      "  Embedded 3700/13829 chunks\n",
      "  Embedded 3750/13829 chunks\n",
      "  Embedded 3800/13829 chunks\n",
      "  Embedded 3850/13829 chunks\n",
      "  Embedded 3900/13829 chunks\n",
      "  Embedded 3950/13829 chunks\n",
      "  Embedded 4000/13829 chunks\n",
      "  Embedded 4050/13829 chunks\n",
      "  Embedded 4100/13829 chunks\n",
      "  Embedded 4150/13829 chunks\n",
      "  Embedded 4200/13829 chunks\n",
      "  Embedded 4250/13829 chunks\n",
      "  Embedded 4300/13829 chunks\n",
      "  Embedded 4350/13829 chunks\n",
      "  Embedded 4400/13829 chunks\n",
      "  Embedded 4450/13829 chunks\n",
      "  Embedded 4500/13829 chunks\n",
      "  Embedded 4550/13829 chunks\n",
      "  Embedded 4600/13829 chunks\n",
      "  Embedded 4650/13829 chunks\n",
      "  Embedded 4700/13829 chunks\n",
      "  Embedded 4750/13829 chunks\n",
      "  Embedded 4800/13829 chunks\n",
      "  Embedded 4850/13829 chunks\n",
      "  Embedded 4900/13829 chunks\n",
      "  Embedded 4950/13829 chunks\n",
      "  Embedded 5000/13829 chunks\n",
      "  Embedded 5050/13829 chunks\n",
      "  Embedded 5100/13829 chunks\n",
      "  Embedded 5150/13829 chunks\n",
      "  Embedded 5200/13829 chunks\n",
      "  Embedded 5250/13829 chunks\n",
      "  Embedded 5300/13829 chunks\n",
      "  Embedded 5350/13829 chunks\n",
      "  Embedded 5400/13829 chunks\n",
      "  Embedded 5450/13829 chunks\n",
      "  Embedded 5500/13829 chunks\n",
      "  Embedded 5550/13829 chunks\n",
      "  Embedded 5600/13829 chunks\n",
      "  Embedded 5650/13829 chunks\n",
      "  Embedded 5700/13829 chunks\n",
      "  Embedded 5750/13829 chunks\n",
      "  Embedded 5800/13829 chunks\n",
      "  Embedded 5850/13829 chunks\n",
      "  Embedded 5900/13829 chunks\n",
      "  Embedded 5950/13829 chunks\n",
      "  Embedded 6000/13829 chunks\n",
      "  Embedded 6050/13829 chunks\n",
      "  Embedded 6100/13829 chunks\n",
      "  Embedded 6150/13829 chunks\n",
      "  Embedded 6200/13829 chunks\n",
      "  Embedded 6250/13829 chunks\n",
      "  Embedded 6300/13829 chunks\n",
      "  Embedded 6350/13829 chunks\n",
      "  Embedded 6400/13829 chunks\n",
      "  Embedded 6450/13829 chunks\n",
      "  Embedded 6500/13829 chunks\n",
      "  Embedded 6550/13829 chunks\n",
      "  Embedded 6600/13829 chunks\n",
      "  Embedded 6650/13829 chunks\n",
      "  Embedded 6700/13829 chunks\n",
      "  Embedded 6750/13829 chunks\n",
      "  Embedded 6800/13829 chunks\n",
      "  Embedded 6850/13829 chunks\n",
      "  Embedded 6900/13829 chunks\n",
      "  Embedded 6950/13829 chunks\n",
      "  Embedded 7000/13829 chunks\n",
      "  Embedded 7050/13829 chunks\n",
      "  Embedded 7100/13829 chunks\n",
      "  Embedded 7150/13829 chunks\n",
      "  Embedded 7200/13829 chunks\n",
      "  Embedded 7250/13829 chunks\n",
      "  Embedded 7300/13829 chunks\n",
      "  Embedded 7350/13829 chunks\n",
      "  Embedded 7400/13829 chunks\n",
      "  Embedded 7450/13829 chunks\n",
      "  Embedded 7500/13829 chunks\n",
      "  Embedded 7550/13829 chunks\n",
      "  Embedded 7600/13829 chunks\n",
      "  Embedded 7650/13829 chunks\n",
      "  Embedded 7700/13829 chunks\n",
      "  Embedded 7750/13829 chunks\n",
      "  Embedded 7800/13829 chunks\n",
      "  Embedded 7850/13829 chunks\n",
      "  Embedded 7900/13829 chunks\n",
      "  Embedded 7950/13829 chunks\n",
      "  Embedded 8000/13829 chunks\n",
      "  Embedded 8050/13829 chunks\n",
      "  Embedded 8100/13829 chunks\n",
      "  Embedded 8150/13829 chunks\n",
      "  Embedded 8200/13829 chunks\n",
      "  Embedded 8250/13829 chunks\n",
      "  Embedded 8300/13829 chunks\n",
      "  Embedded 8350/13829 chunks\n",
      "  Embedded 8400/13829 chunks\n",
      "  Embedded 8450/13829 chunks\n",
      "  Embedded 8500/13829 chunks\n",
      "  Embedded 8550/13829 chunks\n",
      "  Embedded 8600/13829 chunks\n",
      "  Embedded 8650/13829 chunks\n",
      "  Embedded 8700/13829 chunks\n",
      "  Embedded 8750/13829 chunks\n",
      "  Embedded 8800/13829 chunks\n",
      "  Embedded 8850/13829 chunks\n",
      "  Embedded 8900/13829 chunks\n",
      "  Embedded 8950/13829 chunks\n",
      "  Embedded 9000/13829 chunks\n",
      "  Embedded 9050/13829 chunks\n",
      "  Embedded 9100/13829 chunks\n",
      "  Embedded 9150/13829 chunks\n",
      "  Embedded 9200/13829 chunks\n",
      "  Embedded 9250/13829 chunks\n",
      "  Embedded 9300/13829 chunks\n",
      "  Embedded 9350/13829 chunks\n",
      "  Embedded 9400/13829 chunks\n",
      "  Embedded 9450/13829 chunks\n",
      "  Embedded 9500/13829 chunks\n",
      "  Embedded 9550/13829 chunks\n",
      "  Embedded 9600/13829 chunks\n",
      "  Embedded 9650/13829 chunks\n",
      "  Embedded 9700/13829 chunks\n",
      "  Embedded 9750/13829 chunks\n",
      "  Embedded 9800/13829 chunks\n",
      "  Embedded 9850/13829 chunks\n",
      "  Embedded 9900/13829 chunks\n",
      "  Embedded 9950/13829 chunks\n",
      "  Embedded 10000/13829 chunks\n",
      "  Embedded 10050/13829 chunks\n",
      "  Embedded 10100/13829 chunks\n",
      "  Embedded 10150/13829 chunks\n",
      "  Embedded 10200/13829 chunks\n",
      "  Embedded 10250/13829 chunks\n",
      "  Embedded 10300/13829 chunks\n",
      "  Embedded 10350/13829 chunks\n",
      "  Embedded 10400/13829 chunks\n",
      "  Embedded 10450/13829 chunks\n",
      "  Embedded 10500/13829 chunks\n",
      "  Embedded 10550/13829 chunks\n",
      "  Embedded 10600/13829 chunks\n",
      "  Embedded 10650/13829 chunks\n",
      "  Embedded 10700/13829 chunks\n",
      "  Embedded 10750/13829 chunks\n",
      "  Embedded 10800/13829 chunks\n",
      "  Embedded 10850/13829 chunks\n",
      "  Embedded 10900/13829 chunks\n",
      "  Embedded 10950/13829 chunks\n",
      "  Embedded 11000/13829 chunks\n",
      "  Embedded 11050/13829 chunks\n",
      "  Embedded 11100/13829 chunks\n",
      "  Embedded 11150/13829 chunks\n",
      "  Embedded 11200/13829 chunks\n",
      "  Embedded 11250/13829 chunks\n",
      "  Embedded 11300/13829 chunks\n",
      "  Embedded 11350/13829 chunks\n",
      "  Embedded 11400/13829 chunks\n",
      "  Embedded 11450/13829 chunks\n",
      "  Embedded 11500/13829 chunks\n",
      "  Embedded 11550/13829 chunks\n",
      "  Embedded 11600/13829 chunks\n",
      "  Embedded 11650/13829 chunks\n",
      "  Embedded 11700/13829 chunks\n",
      "  Embedded 11750/13829 chunks\n",
      "  Embedded 11800/13829 chunks\n",
      "  Embedded 11850/13829 chunks\n",
      "  Embedded 11900/13829 chunks\n",
      "  Embedded 11950/13829 chunks\n",
      "  Embedded 12000/13829 chunks\n",
      "  Embedded 12050/13829 chunks\n",
      "  Embedded 12100/13829 chunks\n",
      "  Embedded 12150/13829 chunks\n",
      "  Embedded 12200/13829 chunks\n",
      "  Embedded 12250/13829 chunks\n",
      "  Embedded 12300/13829 chunks\n",
      "  Embedded 12350/13829 chunks\n",
      "  Embedded 12400/13829 chunks\n",
      "  Embedded 12450/13829 chunks\n",
      "  Embedded 12500/13829 chunks\n",
      "  Embedded 12550/13829 chunks\n",
      "  Embedded 12600/13829 chunks\n",
      "  Embedded 12650/13829 chunks\n",
      "  Embedded 12700/13829 chunks\n",
      "  Embedded 12750/13829 chunks\n",
      "  Embedded 12800/13829 chunks\n",
      "  Embedded 12850/13829 chunks\n",
      "  Embedded 12900/13829 chunks\n",
      "  Embedded 12950/13829 chunks\n",
      "  Embedded 13000/13829 chunks\n",
      "  Embedded 13050/13829 chunks\n",
      "  Embedded 13100/13829 chunks\n",
      "  Embedded 13150/13829 chunks\n",
      "  Embedded 13200/13829 chunks\n",
      "  Embedded 13250/13829 chunks\n",
      "  Embedded 13300/13829 chunks\n",
      "  Embedded 13350/13829 chunks\n",
      "  Embedded 13400/13829 chunks\n",
      "  Embedded 13450/13829 chunks\n",
      "  Embedded 13500/13829 chunks\n",
      "  Embedded 13550/13829 chunks\n",
      "  Embedded 13600/13829 chunks\n",
      "  Embedded 13650/13829 chunks\n",
      "  Embedded 13700/13829 chunks\n",
      "  Embedded 13750/13829 chunks\n",
      "  Embedded 13800/13829 chunks\n",
      "✓ Generated 13829 vector embeddings\n",
      "\n",
      "================================================================================\n",
      "STEP 4: QUERYING PATIENT QUESTIONS AGAINST NOTE CHUNKS\n",
      "================================================================================\n",
      "✓ Created queries dataframe with 2300 questions\n",
      "✓ Question types: {'yes': 1500, 'numeric': 800}\n",
      "✓ Questions per patient (avg): 23.00\n",
      "Queries to process: 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 2300/2300 [01:45<00:00, 21.71query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved all prompts to fixed_RAG_prompt_save_c_50_k_15.csv\n",
      "fixed_RAG_prompt_save_c_100_k_8.csv\n",
      "Loading Bio_ClinicalBERT for embeddings...\n",
      "================================================================================\n",
      "STEP 1: EXTRACT UNIQUE MEDICAL NOTES\n",
      "================================================================================\n",
      "✓ Extracted 100 unique notes\n",
      "✓ From 100 unique patients\n",
      "\n",
      "================================================================================\n",
      "STEP 2: CREATING CHUNKS FROM NOTES\n",
      "================================================================================\n",
      "✓ Created 6774 chunks from 100 notes\n",
      "✓ Average chunks per note: 67.74\n",
      "\n",
      "================================================================================\n",
      "STEP 3: CREATING EMBEDDINGS FOR CHUNKS\n",
      "================================================================================\n",
      "  Embedded 50/6774 chunks\n",
      "  Embedded 100/6774 chunks\n",
      "  Embedded 150/6774 chunks\n",
      "  Embedded 200/6774 chunks\n",
      "  Embedded 250/6774 chunks\n",
      "  Embedded 300/6774 chunks\n",
      "  Embedded 350/6774 chunks\n",
      "  Embedded 400/6774 chunks\n",
      "  Embedded 450/6774 chunks\n",
      "  Embedded 500/6774 chunks\n",
      "  Embedded 550/6774 chunks\n",
      "  Embedded 600/6774 chunks\n",
      "  Embedded 650/6774 chunks\n",
      "  Embedded 700/6774 chunks\n",
      "  Embedded 750/6774 chunks\n",
      "  Embedded 800/6774 chunks\n",
      "  Embedded 850/6774 chunks\n",
      "  Embedded 900/6774 chunks\n",
      "  Embedded 950/6774 chunks\n",
      "  Embedded 1000/6774 chunks\n",
      "  Embedded 1050/6774 chunks\n",
      "  Embedded 1100/6774 chunks\n",
      "  Embedded 1150/6774 chunks\n",
      "  Embedded 1200/6774 chunks\n",
      "  Embedded 1250/6774 chunks\n",
      "  Embedded 1300/6774 chunks\n",
      "  Embedded 1350/6774 chunks\n",
      "  Embedded 1400/6774 chunks\n",
      "  Embedded 1450/6774 chunks\n",
      "  Embedded 1500/6774 chunks\n",
      "  Embedded 1550/6774 chunks\n",
      "  Embedded 1600/6774 chunks\n",
      "  Embedded 1650/6774 chunks\n",
      "  Embedded 1700/6774 chunks\n",
      "  Embedded 1750/6774 chunks\n",
      "  Embedded 1800/6774 chunks\n",
      "  Embedded 1850/6774 chunks\n",
      "  Embedded 1900/6774 chunks\n",
      "  Embedded 1950/6774 chunks\n",
      "  Embedded 2000/6774 chunks\n",
      "  Embedded 2050/6774 chunks\n",
      "  Embedded 2100/6774 chunks\n",
      "  Embedded 2150/6774 chunks\n",
      "  Embedded 2200/6774 chunks\n",
      "  Embedded 2250/6774 chunks\n",
      "  Embedded 2300/6774 chunks\n",
      "  Embedded 2350/6774 chunks\n",
      "  Embedded 2400/6774 chunks\n",
      "  Embedded 2450/6774 chunks\n",
      "  Embedded 2500/6774 chunks\n",
      "  Embedded 2550/6774 chunks\n",
      "  Embedded 2600/6774 chunks\n",
      "  Embedded 2650/6774 chunks\n",
      "  Embedded 2700/6774 chunks\n",
      "  Embedded 2750/6774 chunks\n",
      "  Embedded 2800/6774 chunks\n",
      "  Embedded 2850/6774 chunks\n",
      "  Embedded 2900/6774 chunks\n",
      "  Embedded 2950/6774 chunks\n",
      "  Embedded 3000/6774 chunks\n",
      "  Embedded 3050/6774 chunks\n",
      "  Embedded 3100/6774 chunks\n",
      "  Embedded 3150/6774 chunks\n",
      "  Embedded 3200/6774 chunks\n",
      "  Embedded 3250/6774 chunks\n",
      "  Embedded 3300/6774 chunks\n",
      "  Embedded 3350/6774 chunks\n",
      "  Embedded 3400/6774 chunks\n",
      "  Embedded 3450/6774 chunks\n",
      "  Embedded 3500/6774 chunks\n",
      "  Embedded 3550/6774 chunks\n",
      "  Embedded 3600/6774 chunks\n",
      "  Embedded 3650/6774 chunks\n",
      "  Embedded 3700/6774 chunks\n",
      "  Embedded 3750/6774 chunks\n",
      "  Embedded 3800/6774 chunks\n",
      "  Embedded 3850/6774 chunks\n",
      "  Embedded 3900/6774 chunks\n",
      "  Embedded 3950/6774 chunks\n",
      "  Embedded 4000/6774 chunks\n",
      "  Embedded 4050/6774 chunks\n",
      "  Embedded 4100/6774 chunks\n",
      "  Embedded 4150/6774 chunks\n",
      "  Embedded 4200/6774 chunks\n",
      "  Embedded 4250/6774 chunks\n",
      "  Embedded 4300/6774 chunks\n",
      "  Embedded 4350/6774 chunks\n",
      "  Embedded 4400/6774 chunks\n",
      "  Embedded 4450/6774 chunks\n",
      "  Embedded 4500/6774 chunks\n",
      "  Embedded 4550/6774 chunks\n",
      "  Embedded 4600/6774 chunks\n",
      "  Embedded 4650/6774 chunks\n",
      "  Embedded 4700/6774 chunks\n",
      "  Embedded 4750/6774 chunks\n",
      "  Embedded 4800/6774 chunks\n",
      "  Embedded 4850/6774 chunks\n",
      "  Embedded 4900/6774 chunks\n",
      "  Embedded 4950/6774 chunks\n",
      "  Embedded 5000/6774 chunks\n",
      "  Embedded 5050/6774 chunks\n",
      "  Embedded 5100/6774 chunks\n",
      "  Embedded 5150/6774 chunks\n",
      "  Embedded 5200/6774 chunks\n",
      "  Embedded 5250/6774 chunks\n",
      "  Embedded 5300/6774 chunks\n",
      "  Embedded 5350/6774 chunks\n",
      "  Embedded 5400/6774 chunks\n",
      "  Embedded 5450/6774 chunks\n",
      "  Embedded 5500/6774 chunks\n",
      "  Embedded 5550/6774 chunks\n",
      "  Embedded 5600/6774 chunks\n",
      "  Embedded 5650/6774 chunks\n",
      "  Embedded 5700/6774 chunks\n",
      "  Embedded 5750/6774 chunks\n",
      "  Embedded 5800/6774 chunks\n",
      "  Embedded 5850/6774 chunks\n",
      "  Embedded 5900/6774 chunks\n",
      "  Embedded 5950/6774 chunks\n",
      "  Embedded 6000/6774 chunks\n",
      "  Embedded 6050/6774 chunks\n",
      "  Embedded 6100/6774 chunks\n",
      "  Embedded 6150/6774 chunks\n",
      "  Embedded 6200/6774 chunks\n",
      "  Embedded 6250/6774 chunks\n",
      "  Embedded 6300/6774 chunks\n",
      "  Embedded 6350/6774 chunks\n",
      "  Embedded 6400/6774 chunks\n",
      "  Embedded 6450/6774 chunks\n",
      "  Embedded 6500/6774 chunks\n",
      "  Embedded 6550/6774 chunks\n",
      "  Embedded 6600/6774 chunks\n",
      "  Embedded 6650/6774 chunks\n",
      "  Embedded 6700/6774 chunks\n",
      "  Embedded 6750/6774 chunks\n",
      "✓ Generated 6774 vector embeddings\n",
      "\n",
      "================================================================================\n",
      "STEP 4: QUERYING PATIENT QUESTIONS AGAINST NOTE CHUNKS\n",
      "================================================================================\n",
      "✓ Created queries dataframe with 2300 questions\n",
      "✓ Question types: {'yes': 1500, 'numeric': 800}\n",
      "✓ Questions per patient (avg): 23.00\n",
      "Queries to process: 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 2300/2300 [01:39<00:00, 23.23query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved all prompts to fixed_RAG_prompt_save_c_100_k_8.csv\n",
      "fixed_RAG_prompt_save_c_150_k_5.csv\n",
      "Loading Bio_ClinicalBERT for embeddings...\n",
      "================================================================================\n",
      "STEP 1: EXTRACT UNIQUE MEDICAL NOTES\n",
      "================================================================================\n",
      "✓ Extracted 100 unique notes\n",
      "✓ From 100 unique patients\n",
      "\n",
      "================================================================================\n",
      "STEP 2: CREATING CHUNKS FROM NOTES\n",
      "================================================================================\n",
      "✓ Created 4226 chunks from 100 notes\n",
      "✓ Average chunks per note: 42.26\n",
      "\n",
      "================================================================================\n",
      "STEP 3: CREATING EMBEDDINGS FOR CHUNKS\n",
      "================================================================================\n",
      "  Embedded 50/4226 chunks\n",
      "  Embedded 100/4226 chunks\n",
      "  Embedded 150/4226 chunks\n",
      "  Embedded 200/4226 chunks\n",
      "  Embedded 250/4226 chunks\n",
      "  Embedded 300/4226 chunks\n",
      "  Embedded 350/4226 chunks\n",
      "  Embedded 400/4226 chunks\n",
      "  Embedded 450/4226 chunks\n",
      "  Embedded 500/4226 chunks\n",
      "  Embedded 550/4226 chunks\n",
      "  Embedded 600/4226 chunks\n",
      "  Embedded 650/4226 chunks\n",
      "  Embedded 700/4226 chunks\n",
      "  Embedded 750/4226 chunks\n",
      "  Embedded 800/4226 chunks\n",
      "  Embedded 850/4226 chunks\n",
      "  Embedded 900/4226 chunks\n",
      "  Embedded 950/4226 chunks\n",
      "  Embedded 1000/4226 chunks\n",
      "  Embedded 1050/4226 chunks\n",
      "  Embedded 1100/4226 chunks\n",
      "  Embedded 1150/4226 chunks\n",
      "  Embedded 1200/4226 chunks\n",
      "  Embedded 1250/4226 chunks\n",
      "  Embedded 1300/4226 chunks\n",
      "  Embedded 1350/4226 chunks\n",
      "  Embedded 1400/4226 chunks\n",
      "  Embedded 1450/4226 chunks\n",
      "  Embedded 1500/4226 chunks\n",
      "  Embedded 1550/4226 chunks\n",
      "  Embedded 1600/4226 chunks\n",
      "  Embedded 1650/4226 chunks\n",
      "  Embedded 1700/4226 chunks\n",
      "  Embedded 1750/4226 chunks\n",
      "  Embedded 1800/4226 chunks\n",
      "  Embedded 1850/4226 chunks\n",
      "  Embedded 1900/4226 chunks\n",
      "  Embedded 1950/4226 chunks\n",
      "  Embedded 2000/4226 chunks\n",
      "  Embedded 2050/4226 chunks\n",
      "  Embedded 2100/4226 chunks\n",
      "  Embedded 2150/4226 chunks\n",
      "  Embedded 2200/4226 chunks\n",
      "  Embedded 2250/4226 chunks\n",
      "  Embedded 2300/4226 chunks\n",
      "  Embedded 2350/4226 chunks\n",
      "  Embedded 2400/4226 chunks\n",
      "  Embedded 2450/4226 chunks\n",
      "  Embedded 2500/4226 chunks\n",
      "  Embedded 2550/4226 chunks\n",
      "  Embedded 2600/4226 chunks\n",
      "  Embedded 2650/4226 chunks\n",
      "  Embedded 2700/4226 chunks\n",
      "  Embedded 2750/4226 chunks\n",
      "  Embedded 2800/4226 chunks\n",
      "  Embedded 2850/4226 chunks\n",
      "  Embedded 2900/4226 chunks\n",
      "  Embedded 2950/4226 chunks\n",
      "  Embedded 3000/4226 chunks\n",
      "  Embedded 3050/4226 chunks\n",
      "  Embedded 3100/4226 chunks\n",
      "  Embedded 3150/4226 chunks\n",
      "  Embedded 3200/4226 chunks\n",
      "  Embedded 3250/4226 chunks\n",
      "  Embedded 3300/4226 chunks\n",
      "  Embedded 3350/4226 chunks\n",
      "  Embedded 3400/4226 chunks\n",
      "  Embedded 3450/4226 chunks\n",
      "  Embedded 3500/4226 chunks\n",
      "  Embedded 3550/4226 chunks\n",
      "  Embedded 3600/4226 chunks\n",
      "  Embedded 3650/4226 chunks\n",
      "  Embedded 3700/4226 chunks\n",
      "  Embedded 3750/4226 chunks\n",
      "  Embedded 3800/4226 chunks\n",
      "  Embedded 3850/4226 chunks\n",
      "  Embedded 3900/4226 chunks\n",
      "  Embedded 3950/4226 chunks\n",
      "  Embedded 4000/4226 chunks\n",
      "  Embedded 4050/4226 chunks\n",
      "  Embedded 4100/4226 chunks\n",
      "  Embedded 4150/4226 chunks\n",
      "  Embedded 4200/4226 chunks\n",
      "✓ Generated 4226 vector embeddings\n",
      "\n",
      "================================================================================\n",
      "STEP 4: QUERYING PATIENT QUESTIONS AGAINST NOTE CHUNKS\n",
      "================================================================================\n",
      "✓ Created queries dataframe with 2300 questions\n",
      "✓ Question types: {'yes': 1500, 'numeric': 800}\n",
      "✓ Questions per patient (avg): 23.00\n",
      "Queries to process: 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 2300/2300 [01:52<00:00, 20.53query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved all prompts to fixed_RAG_prompt_save_c_150_k_5.csv\n",
      "fixed_RAG_prompt_save_c_200_k_4.csv\n",
      "Loading Bio_ClinicalBERT for embeddings...\n",
      "================================================================================\n",
      "STEP 1: EXTRACT UNIQUE MEDICAL NOTES\n",
      "================================================================================\n",
      "✓ Extracted 100 unique notes\n",
      "✓ From 100 unique patients\n",
      "\n",
      "================================================================================\n",
      "STEP 2: CREATING CHUNKS FROM NOTES\n",
      "================================================================================\n",
      "✓ Created 2976 chunks from 100 notes\n",
      "✓ Average chunks per note: 29.76\n",
      "\n",
      "================================================================================\n",
      "STEP 3: CREATING EMBEDDINGS FOR CHUNKS\n",
      "================================================================================\n",
      "  Embedded 50/2976 chunks\n",
      "  Embedded 100/2976 chunks\n",
      "  Embedded 150/2976 chunks\n",
      "  Embedded 200/2976 chunks\n",
      "  Embedded 250/2976 chunks\n",
      "  Embedded 300/2976 chunks\n",
      "  Embedded 350/2976 chunks\n",
      "  Embedded 400/2976 chunks\n",
      "  Embedded 450/2976 chunks\n",
      "  Embedded 500/2976 chunks\n",
      "  Embedded 550/2976 chunks\n",
      "  Embedded 600/2976 chunks\n",
      "  Embedded 650/2976 chunks\n",
      "  Embedded 700/2976 chunks\n",
      "  Embedded 750/2976 chunks\n",
      "  Embedded 800/2976 chunks\n",
      "  Embedded 850/2976 chunks\n",
      "  Embedded 900/2976 chunks\n",
      "  Embedded 950/2976 chunks\n",
      "  Embedded 1000/2976 chunks\n",
      "  Embedded 1050/2976 chunks\n",
      "  Embedded 1100/2976 chunks\n",
      "  Embedded 1150/2976 chunks\n",
      "  Embedded 1200/2976 chunks\n",
      "  Embedded 1250/2976 chunks\n",
      "  Embedded 1300/2976 chunks\n",
      "  Embedded 1350/2976 chunks\n",
      "  Embedded 1400/2976 chunks\n",
      "  Embedded 1450/2976 chunks\n",
      "  Embedded 1500/2976 chunks\n",
      "  Embedded 1550/2976 chunks\n",
      "  Embedded 1600/2976 chunks\n",
      "  Embedded 1650/2976 chunks\n",
      "  Embedded 1700/2976 chunks\n",
      "  Embedded 1750/2976 chunks\n",
      "  Embedded 1800/2976 chunks\n",
      "  Embedded 1850/2976 chunks\n",
      "  Embedded 1900/2976 chunks\n",
      "  Embedded 1950/2976 chunks\n",
      "  Embedded 2000/2976 chunks\n",
      "  Embedded 2050/2976 chunks\n",
      "  Embedded 2100/2976 chunks\n",
      "  Embedded 2150/2976 chunks\n",
      "  Embedded 2200/2976 chunks\n",
      "  Embedded 2250/2976 chunks\n",
      "  Embedded 2300/2976 chunks\n",
      "  Embedded 2350/2976 chunks\n",
      "  Embedded 2400/2976 chunks\n",
      "  Embedded 2450/2976 chunks\n",
      "  Embedded 2500/2976 chunks\n",
      "  Embedded 2550/2976 chunks\n",
      "  Embedded 2600/2976 chunks\n",
      "  Embedded 2650/2976 chunks\n",
      "  Embedded 2700/2976 chunks\n",
      "  Embedded 2750/2976 chunks\n",
      "  Embedded 2800/2976 chunks\n",
      "  Embedded 2850/2976 chunks\n",
      "  Embedded 2900/2976 chunks\n",
      "  Embedded 2950/2976 chunks\n",
      "✓ Generated 2976 vector embeddings\n",
      "\n",
      "================================================================================\n",
      "STEP 4: QUERYING PATIENT QUESTIONS AGAINST NOTE CHUNKS\n",
      "================================================================================\n",
      "✓ Created queries dataframe with 2300 questions\n",
      "✓ Question types: {'yes': 1500, 'numeric': 800}\n",
      "✓ Questions per patient (avg): 23.00\n",
      "Queries to process: 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 2300/2300 [01:27<00:00, 26.31query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved all prompts to fixed_RAG_prompt_save_c_200_k_4.csv\n",
      "fixed_RAG_prompt_save_c_300_k_2.csv\n",
      "Loading Bio_ClinicalBERT for embeddings...\n",
      "================================================================================\n",
      "STEP 1: EXTRACT UNIQUE MEDICAL NOTES\n",
      "================================================================================\n",
      "✓ Extracted 100 unique notes\n",
      "✓ From 100 unique patients\n",
      "\n",
      "================================================================================\n",
      "STEP 2: CREATING CHUNKS FROM NOTES\n",
      "================================================================================\n",
      "✓ Created 1850 chunks from 100 notes\n",
      "✓ Average chunks per note: 18.50\n",
      "\n",
      "================================================================================\n",
      "STEP 3: CREATING EMBEDDINGS FOR CHUNKS\n",
      "================================================================================\n",
      "  Embedded 50/1850 chunks\n",
      "  Embedded 100/1850 chunks\n",
      "  Embedded 150/1850 chunks\n",
      "  Embedded 200/1850 chunks\n",
      "  Embedded 250/1850 chunks\n",
      "  Embedded 300/1850 chunks\n",
      "  Embedded 350/1850 chunks\n",
      "  Embedded 400/1850 chunks\n",
      "  Embedded 450/1850 chunks\n",
      "  Embedded 500/1850 chunks\n",
      "  Embedded 550/1850 chunks\n",
      "  Embedded 600/1850 chunks\n",
      "  Embedded 650/1850 chunks\n",
      "  Embedded 700/1850 chunks\n",
      "  Embedded 750/1850 chunks\n",
      "  Embedded 800/1850 chunks\n",
      "  Embedded 850/1850 chunks\n",
      "  Embedded 900/1850 chunks\n",
      "  Embedded 950/1850 chunks\n",
      "  Embedded 1000/1850 chunks\n",
      "  Embedded 1050/1850 chunks\n",
      "  Embedded 1100/1850 chunks\n",
      "  Embedded 1150/1850 chunks\n",
      "  Embedded 1200/1850 chunks\n",
      "  Embedded 1250/1850 chunks\n",
      "  Embedded 1300/1850 chunks\n",
      "  Embedded 1350/1850 chunks\n",
      "  Embedded 1400/1850 chunks\n",
      "  Embedded 1450/1850 chunks\n",
      "  Embedded 1500/1850 chunks\n",
      "  Embedded 1550/1850 chunks\n",
      "  Embedded 1600/1850 chunks\n",
      "  Embedded 1650/1850 chunks\n",
      "  Embedded 1700/1850 chunks\n",
      "  Embedded 1750/1850 chunks\n",
      "  Embedded 1800/1850 chunks\n",
      "  Embedded 1850/1850 chunks\n",
      "✓ Generated 1850 vector embeddings\n",
      "\n",
      "================================================================================\n",
      "STEP 4: QUERYING PATIENT QUESTIONS AGAINST NOTE CHUNKS\n",
      "================================================================================\n",
      "✓ Created queries dataframe with 2300 questions\n",
      "✓ Question types: {'yes': 1500, 'numeric': 800}\n",
      "✓ Questions per patient (avg): 23.00\n",
      "Queries to process: 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 2300/2300 [01:23<00:00, 27.40query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved all prompts to fixed_RAG_prompt_save_c_300_k_2.csv\n",
      "fixed_RAG_prompt_save_c_400_k_2.csv\n",
      "Loading Bio_ClinicalBERT for embeddings...\n",
      "================================================================================\n",
      "STEP 1: EXTRACT UNIQUE MEDICAL NOTES\n",
      "================================================================================\n",
      "✓ Extracted 100 unique notes\n",
      "✓ From 100 unique patients\n",
      "\n",
      "================================================================================\n",
      "STEP 2: CREATING CHUNKS FROM NOTES\n",
      "================================================================================\n",
      "✓ Created 1346 chunks from 100 notes\n",
      "✓ Average chunks per note: 13.46\n",
      "\n",
      "================================================================================\n",
      "STEP 3: CREATING EMBEDDINGS FOR CHUNKS\n",
      "================================================================================\n",
      "  Embedded 50/1346 chunks\n",
      "  Embedded 100/1346 chunks\n",
      "  Embedded 150/1346 chunks\n",
      "  Embedded 200/1346 chunks\n",
      "  Embedded 250/1346 chunks\n",
      "  Embedded 300/1346 chunks\n",
      "  Embedded 350/1346 chunks\n",
      "  Embedded 400/1346 chunks\n",
      "  Embedded 450/1346 chunks\n",
      "  Embedded 500/1346 chunks\n",
      "  Embedded 550/1346 chunks\n",
      "  Embedded 600/1346 chunks\n",
      "  Embedded 650/1346 chunks\n",
      "  Embedded 700/1346 chunks\n",
      "  Embedded 750/1346 chunks\n",
      "  Embedded 800/1346 chunks\n",
      "  Embedded 850/1346 chunks\n",
      "  Embedded 900/1346 chunks\n",
      "  Embedded 950/1346 chunks\n",
      "  Embedded 1000/1346 chunks\n",
      "  Embedded 1050/1346 chunks\n",
      "  Embedded 1100/1346 chunks\n",
      "  Embedded 1150/1346 chunks\n",
      "  Embedded 1200/1346 chunks\n",
      "  Embedded 1250/1346 chunks\n",
      "  Embedded 1300/1346 chunks\n",
      "✓ Generated 1346 vector embeddings\n",
      "\n",
      "================================================================================\n",
      "STEP 4: QUERYING PATIENT QUESTIONS AGAINST NOTE CHUNKS\n",
      "================================================================================\n",
      "✓ Created queries dataframe with 2300 questions\n",
      "✓ Question types: {'yes': 1500, 'numeric': 800}\n",
      "✓ Questions per patient (avg): 23.00\n",
      "Queries to process: 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 2300/2300 [01:38<00:00, 23.25query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved all prompts to fixed_RAG_prompt_save_c_400_k_2.csv\n",
      "fixed_RAG_prompt_save_c_500_k_1.csv\n",
      "Loading Bio_ClinicalBERT for embeddings...\n",
      "================================================================================\n",
      "STEP 1: EXTRACT UNIQUE MEDICAL NOTES\n",
      "================================================================================\n",
      "✓ Extracted 100 unique notes\n",
      "✓ From 100 unique patients\n",
      "\n",
      "================================================================================\n",
      "STEP 2: CREATING CHUNKS FROM NOTES\n",
      "================================================================================\n",
      "✓ Created 1048 chunks from 100 notes\n",
      "✓ Average chunks per note: 10.48\n",
      "\n",
      "================================================================================\n",
      "STEP 3: CREATING EMBEDDINGS FOR CHUNKS\n",
      "================================================================================\n",
      "  Embedded 50/1048 chunks\n",
      "  Embedded 100/1048 chunks\n",
      "  Embedded 150/1048 chunks\n",
      "  Embedded 200/1048 chunks\n",
      "  Embedded 250/1048 chunks\n",
      "  Embedded 300/1048 chunks\n",
      "  Embedded 350/1048 chunks\n",
      "  Embedded 400/1048 chunks\n",
      "  Embedded 450/1048 chunks\n",
      "  Embedded 500/1048 chunks\n",
      "  Embedded 550/1048 chunks\n",
      "  Embedded 600/1048 chunks\n",
      "  Embedded 650/1048 chunks\n",
      "  Embedded 700/1048 chunks\n",
      "  Embedded 750/1048 chunks\n",
      "  Embedded 800/1048 chunks\n",
      "  Embedded 850/1048 chunks\n",
      "  Embedded 900/1048 chunks\n",
      "  Embedded 950/1048 chunks\n",
      "  Embedded 1000/1048 chunks\n",
      "✓ Generated 1048 vector embeddings\n",
      "\n",
      "================================================================================\n",
      "STEP 4: QUERYING PATIENT QUESTIONS AGAINST NOTE CHUNKS\n",
      "================================================================================\n",
      "✓ Created queries dataframe with 2300 questions\n",
      "✓ Question types: {'yes': 1500, 'numeric': 800}\n",
      "✓ Questions per patient (avg): 23.00\n",
      "Queries to process: 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating queries: 100%|██████████| 2300/2300 [01:23<00:00, 27.66query/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Saved all prompts to fixed_RAG_prompt_save_c_500_k_1.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv('./annotated_apixaban_combined_fixed.csv')\n",
    "\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"Unique notes: {df['note_id'].nunique()}\")\n",
    "    print(f\"Unique patients: {df['hadm_id'].nunique()}\")\n",
    "    iter_list = [[50,15],[100,8],[150,5],[200,4],[300,2],[400,2],[500,1]]\n",
    "    # iter_list = [50,100,150,200,250]\n",
    "    kw_top_k = 15\n",
    "    sem_top_k = 15\n",
    "    alpha = 0.3\n",
    "    for chunk,final_top_k in iter_list:\n",
    "        # Initialize RAG system\n",
    "        f_name = f'fixed_RAG_prompt_save_c_{chunk}_k_{final_top_k}.csv'\n",
    "        print(f_name)\n",
    "        rag_system = MedicalNotesRAG(df, chunk_size=chunk, name_file=f_name)\n",
    "\n",
    "        # Execute pipeline\n",
    "        print(\"=\" * 80)\n",
    "        print(\"STEP 1: EXTRACT UNIQUE MEDICAL NOTES\")\n",
    "        print(\"=\" * 80)\n",
    "        notes = rag_system.extract_notes()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 2: CREATING CHUNKS FROM NOTES\")\n",
    "        print(\"=\"*80)\n",
    "        chunks = rag_system.create_chunks()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 3: CREATING EMBEDDINGS FOR CHUNKS\")\n",
    "        print(\"=\"*80)\n",
    "        vector_index = rag_system.create_embeddings()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"STEP 4: QUERYING PATIENT QUESTIONS AGAINST NOTE CHUNKS\")\n",
    "        print(\"=\"*80)\n",
    "        queries_dataframe = rag_system.create_queries_dataframe()\n",
    "        \n",
    "        prompt = rag_system.save_prompt(kw_top_k=kw_top_k,\n",
    "                                        sem_top_k= sem_top_k,\n",
    "                                        alpha = alpha,\n",
    "                                        final_top_k=final_top_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
